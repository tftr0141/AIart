{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tftr0141/AIart/blob/main/imagic/Imagic_Stable_Diffusion_Lisa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 概要\n",
        "\n",
        "Imagic Stable Diffusion を動かすためのノートブックです\n",
        "\n",
        "画像の一部だけをうまい具合に加工してくれます\n",
        "\n",
        "* img2imgやinpaintingとの比較\n",
        "  * メリット\n",
        "    * Imagicの方が元の画像の構図や特徴を活かしてくれます\n",
        "    * 上手くやるとポーズの変更など、大きな可能もできそうです\n",
        "  * デメリット\n",
        "    * 画像変換の準備（学習データの用意）に時間がかかります\n",
        "      * 標準設定で20分～30分程度\n",
        "    * パラメータの調整が現状は手作業なので手間がかかります\n",
        "\n",
        "\n",
        "* 参考\n",
        "  * Imagic Stable Diffusionの解説がわかりやすい記事\n",
        "    * https://birdmanikioishota.blog.fc2.com/blog-entry-12.html\n",
        "  * このノートブックを作るのに参考にしたノートブック\n",
        "    * https://colab.research.google.com/github/ShivamShrirao/diffusers/blob/main/examples/imagic/Imagic_Stable_Diffusion.ipynb\n",
        "  * 元レポジトリなど\n",
        "    * https://github.com/ShivamShrirao/diffusers/tree/main/examples/imagic\n",
        "    "
      ],
      "metadata": {
        "id": "l0aLXlTwcYE0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnTMyW41cC1E"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLWXPZqjsZVV",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown ## 1. 必要なパッケージのインストール(Install Requirements)\n",
        "!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/examples/imagic/train_imagic.py\n",
        "#%pip install -qq git+https://github.com/ShivamShrirao/diffusers\n",
        "%pip install -qq git+https://github.com/ShivamShrirao/diffusers@d34dcaa3c1913c7b794f8438c7c3c5ded265fa48\n",
        "%pip install -q -U --pre triton\n",
        "%pip install -q accelerate==0.12.0 transformers ftfy bitsandbytes gradio\n",
        "\n",
        "\n",
        "#Install xformers from precompiled wheel.\n",
        "%pip install -q https://github.com/metrolobo/xformers_wheels/releases/download/1d31a3ac_various_6/xformers-0.0.14.dev0-cp37-cp37m-linux_x86_64.whl\n",
        "# These were compiled on Tesla T4, should also work on P100, thanks to https://github.com/metrolobo\n",
        "\n",
        "# If precompiled wheels don't work, install it with the following command. It will take around 40 minutes to compile.\n",
        "# %pip install git+https://github.com/facebookresearch/xformers@1d31a3a#egg=xformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "y4lqqWT_uxD2"
      },
      "outputs": [],
      "source": [
        "#@title 2.HuggingFaceへのログイン（Login to HuggingFace 🤗）\n",
        "#@markdown Stable-Diffusionを以外のモデルを使う場合でも必要です\n",
        "#@markdown \n",
        "#@markdown WaifuDiffusionなど別モデルを使う場合は不要です\n",
        "#You need to accept the model license before downloading or using the Stable Diffusion weights. Please, visit the [model card](https://huggingface.co/CompVis/stable-diffusion-v1-4), read the license and tick the checkbox if you agree. You have to be a registered user in 🤗 Hugging Face Hub, and you'll also need to use an access token for the code to work.\n",
        "from huggingface_hub import notebook_login\n",
        "!git config --global credential.helper store\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0NV324ZcL9L"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Rxg0y5MBudmd"
      },
      "outputs": [],
      "source": [
        "#@markdown ## 3.設定\n",
        "MODEL_NAME = \"Nilaier/Waifu-Diffusers\" #@param [\"hakurei/waifu-diffusion\", \"Nilaier/Waifu-Diffusers\",\"CompVis/stable-diffusion-v1-4\", \"naclbit/trinart_stable_diffusion_v2,diffusers-115k\", \"naclbit/trinart_stable_diffusion_v2,diffusers-95k\", \"naclbit/trinart_stable_diffusion_v2,diffusers-60k\"] {allow-input: true}\n",
        "#@markdown 利用するモデルを選択\n",
        "#@markdown   * CompVis/stable-diffusion-v1-4 → 素のStableDiffusion\n",
        "#@markdown   * hakurei/waifu-diffusion → Waifu-Diffusion 1.2\n",
        "#@markdown   * Nilaier/Waifu-Diffusers → Waifu-Diffusion 1.3\n",
        "#@markdown ----\n",
        "TARGET_TEXT = \"white background, girl wearing glasses\" #@param {type:\"string\"}\n",
        "#@markdown 変更したい絵を書いて下さい\n",
        "#@markdown * 例\n",
        "#@markdown   * 写真の犬を走らせたい→a dog running\n",
        "#@markdown   * 女の子にメガネをかけさせたい→girl wearing glasses\n",
        "#@markdown     * beautifulとかkawaiiとか装飾語をつけたほうが良いのかは未調査だけど、多分つけたほうが良さそう\n",
        "#@markdown ----\n",
        "\n",
        "emb_train_steps = 500 #@param {type:\"number\"}\n",
        "max_train_steps = 1000 #@param {type:\"number\"}\n",
        "#@markdown * 学習用パラメータ。ツールのデフォルト値は500と1000\n",
        "#@markdown * 増やすと精度が高くなる分、時間がかかります\n",
        "#@markdown   * メガネ追加の検証時は、1000・2000にしたほうが安定したので、時間に余裕があるなら増やした方安全かもです\n",
        "#@markdown   * 500/1000で学習に20分、1000/2000で40分程度かかります\n",
        "#@markdown ----\n",
        "#@markdown * 学習用パラメータ。デフォルトは1000\n",
        "#@markdown ----\n",
        "\n",
        "IMG_OUTPUT_PATH = \"/content/drive/MyDrive/stable_diffusion_weights/results\" #@param {type:\"string\"}\n",
        "#@markdown * 結果画像の保存先\n",
        "#@markdown ----\n",
        "\n",
        "save_to_gdrive = False #@param {type:\"boolean\"}\n",
        "#@markdown * GoogleDriveにモデルデータを保存させたい場合は、チェックを付けて下さい（基本は無しでOK）\n",
        "#@markdown ----\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "OUTPUT_DIR = \"stable_diffusion_weights/imagic\" #@param {type:\"string\"}\n",
        "#@markdown * モデルデータの保存先（基本はそのままでOK）\n",
        "#@markdown ----\n",
        "if save_to_gdrive:\n",
        "    OUTPUT_DIR = \"/content/drive/MyDrive/\" + OUTPUT_DIR\n",
        "else:\n",
        "    OUTPUT_DIR = \"/content/\" + OUTPUT_DIR\n",
        "\n",
        "print(f\"[*] Weights will be saved at {OUTPUT_DIR}\")\n",
        "!mkdir -p $OUTPUT_DIR\n",
        "!mkdir -p $IMG_OUTPUT_PATH\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fe-GgtnUVO_e"
      },
      "outputs": [],
      "source": [
        "#@markdown ## 4.元画像をアップロード\n",
        "#@markdown 512x512以外で動くかは確認してません\n",
        "\n",
        "import os\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "uploaded = files.upload()\n",
        "for filename in uploaded.keys():\n",
        "    INPUT_IMAGE = os.path.join(OUTPUT_DIR, filename)\n",
        "    shutil.move(filename, INPUT_IMAGE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjcSXTp-u-Eg",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown ## 5.学習実行\n",
        "#@markdown 標準設定で20～30分程度かかります\n",
        "!accelerate launch train_imagic.py \\\n",
        "  --pretrained_model_name_or_path=$MODEL_NAME \\\n",
        "  --output_dir=$OUTPUT_DIR \\\n",
        "  --input_image=$INPUT_IMAGE \\\n",
        "  --target_text=\"{TARGET_TEXT}\" \\\n",
        "  --seed=3434554 \\\n",
        "  --resolution=512 \\\n",
        "  --mixed_precision=\"fp16\" \\\n",
        "  --use_8bit_adam \\\n",
        "  --gradient_accumulation_steps=1 \\\n",
        "  --emb_learning_rate=1e-3 \\\n",
        "  --learning_rate=1e-6 \\\n",
        "  --emb_train_steps={emb_train_steps} \\\n",
        "  --max_train_steps={max_train_steps}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89Az5NUxOWdy",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown ## （任意）ckptファイルへ変換（AUTOMATIC1111などで利用する場合）\n",
        "!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/scripts/convert_diffusers_to_original_stable_diffusion.py\n",
        "ckpt_path = OUTPUT_DIR + \"/model.ckpt\"\n",
        "\n",
        "half_arg = \"\"\n",
        "fp16 = True \n",
        "if fp16:\n",
        "    half_arg = \"--half\"\n",
        "!python convert_diffusers_to_original_stable_diffusion.py --model_path $OUTPUT_DIR  --checkpoint_path $ckpt_path $half_arg\n",
        "print(f\"[*] Converted ckpt saved at {ckpt_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gW15FjffdTID",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown ## 6. 画像出力準備\n",
        "#@markdown 30秒程度で終わります\n",
        "import os\n",
        "import torch\n",
        "from torch import autocast\n",
        "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
        "from IPython.display import display\n",
        "\n",
        "model_path = OUTPUT_DIR             # If you want to use previously trained model saved in gdrive, replace this with the full path of model in gdrive\n",
        "\n",
        "scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False)\n",
        "pipe = StableDiffusionPipeline.from_pretrained(model_path, scheduler=scheduler, torch_dtype=torch.float16).to(\"cuda\")\n",
        "target_embeddings = torch.load(os.path.join(model_path, \"target_embeddings.pt\")).to(\"cuda\")\n",
        "optimized_embeddings = torch.load(os.path.join(model_path, \"optimized_embeddings.pt\")).to(\"cuda\")\n",
        "g_cuda = torch.Generator(device='cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMCqQ5Tcdsm2",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown ## 7.画像出力UI起動\n",
        "#@markdown 出力結果は GoogleDrive の stable_diffusion_weights/results にも保存されます\n",
        "#@markdown ### 期待する画像を見つけるためのパラメータ調整\n",
        "#@markdown 運が良いと一度で目当ての画像を引ける場合もありますが、パラメータを調整しないと出ない場合も多いです\n",
        "#@markdown \n",
        "#@markdown * 画像が元の画像のままの場合\n",
        "#@markdown   * promptの値を変えて実行し、元の画像と変換後の画像の境目を探しましょう\n",
        "#@markdown   * 境目だと、0.1違うだけで出力結果が大きく違ったりします\n",
        "#@markdown   * こだわるなら0.01単位でこだわっても良さそうなレベル感でした\n",
        "#@markdown   * 最適値は画像やテキストによって大きく異なる感じですが、最適値は0.5～1.5の間ぐらいな気がしてます\n",
        "#@markdown * 画像がテキスト寄りでぼやけてる\n",
        "#@markdown   * guidance_scaleを増やすとハッキリした絵になるかもしれないです\n",
        "#@markdown   * guidance_scaleを増やしすぎると、原型が無くなるので、程々を探してみて下さい\n",
        "#@markdown   * 最適値は条件によって大きく異なり、3でOKの場合もあれば50近くまで増やして安定したケースもありました\n",
        "#@markdown * guidance_scaleを調整してもぼやける場合\n",
        "#@markdown   * stepsを増やすと良くなるケースもありますが、その分描画時間が長くなります\n",
        "#@markdown   * stepsを増やすのは最後の仕上げぐらいが良い気がします\n",
        "#@markdown * それでも上手くいかない場合\n",
        "#@markdown   * 学習ステップ数が足りない可能性があります\n",
        "#@markdown   * emb_train_stepsやmax_train_stepsを倍の値にして上手くいったケースもあるので、学習パラメータを増やして最初からやり直しましょう\n",
        "#@markdown * その他パラメータ\n",
        "#@markdown   * Number of Samplesはあまり増やすとGPUのメモリ不足になるので、扱い注意\n",
        "#@markdown   * wiidth/heightは変えると画像が崩壊しがちなので、これも取り扱い注意です\n",
        "\n",
        "\n",
        "import gradio as gr\n",
        "import datetime\n",
        "def getTimeText():\n",
        "    t_delta = datetime.timedelta(hours=9)\n",
        "    JST = datetime.timezone(t_delta, 'JST')\n",
        "    now = datetime.datetime.now(JST)\n",
        "    return now.strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "def inference(alpha, num_samples, height=512, width=512, num_inference_steps=50, guidance_scale=7.5, seed = 100):\n",
        "    g_cuda.manual_seed(int(seed))\n",
        "    with torch.autocast(\"cuda\"), torch.inference_mode():\n",
        "        edit_embeddings = alpha*target_embeddings + (1-alpha)*optimized_embeddings\n",
        "        images = pipe(\n",
        "                text_embeddings=edit_embeddings, height=int(height), width=int(width),\n",
        "                num_images_per_prompt=int(num_samples),\n",
        "                num_inference_steps=int(num_inference_steps), guidance_scale=guidance_scale,\n",
        "                generator=g_cuda\n",
        "            ).images\n",
        "        now = getTimeText()\n",
        "        i = 0\n",
        "        for image in images:\n",
        "          i = i + 1\n",
        "          save_name = f\"{now}_{i}_{alpha}_{seed}_{num_samples}_{guidance_scale}_{num_inference_steps}.png\"\n",
        "          image.save(os.path.join(IMG_OUTPUT_PATH,save_name))\n",
        "        return images\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            with gr.Row():\n",
        "              alpha = gr.Number(label=\"Prompt\", value=0.9)\n",
        "              seed = gr.Number(label=\"Seed\", value=100)\n",
        "            run = gr.Button(value=\"Generate\")\n",
        "            with gr.Row():\n",
        "                num_samples = gr.Number(label=\"Number of Samples\", value=4)\n",
        "                guidance_scale = gr.Number(label=\"Guidance Scale\", value=3)\n",
        "            num_inference_steps = gr.Slider(label=\"Steps\", value=20, maximum=200)\n",
        "            with gr.Row():\n",
        "                height = gr.Number(label=\"Height\", value=512)\n",
        "                width = gr.Number(label=\"Width\", value=512)\n",
        "        with gr.Column():\n",
        "            gallery = gr.Gallery()\n",
        "\n",
        "    run.click(inference, inputs=[alpha, num_samples, height, width, num_inference_steps, guidance_scale,seed], outputs=gallery)\n",
        "\n",
        "demo.launch(debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ## 8.セッションの切断\n",
        "#@markdown セッションを繋いだままだとクレジットを消費するので、実行が終わったら切断しましょう\n",
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "wBG65pHH4GDk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}